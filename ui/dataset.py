import streamlit as st
import json
import os
import sys
from itertools import product
import glob
from datetime import datetime
import shutil
from PIL import Image
from streamlit_shortcuts import button
import pandas as pd
import base64
from pathlib import Path
import time

# Pull in shared variables (file names, JSON object names, ...)
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.abspath("."))

from ui_common import *

DATASET_DIR = f"{DATA_DIR}/{CFG_dset_svc_name}"
MAX_QUEUE = DTS_max_lbl_queue
MAX_IMAGE = DTS_max_items

# Hide completely the iframe generated by streamlit shortcuts
def hide_shortcut_iframe():
    st.markdown("""
        <style>
        iframe[title="st.iframe"] {
            display: none !important;
            height: 0 !important;
            width: 0 !important;
            visibility: hidden !important;
        }
        </style>
    """, unsafe_allow_html=True)

# Generate all permutations of channels and objects
def generate_permutations(channels, objects):
   return list(product(channels, objects))

def remove_matching(pattern):
    for name in glob.glob(pattern):
        if os.path.isfile(name):
            os.remove(name)
        else:
            shutil.rmtree(name)

def next_image(image_dirs, image_index, count=1):
    if image_index <= image_dirs[-1] - count: image_index += count
    else: image_index = image_dirs[0]
    return image_index

def prev_image(image_dirs, image_index, count=1):
    if image_index >= image_dirs[0] + count: image_index -= count
    else: image_index = image_dirs[-1]
    return image_index

def image_browsing(subpath, start_index=0):
    dataset_dir = f"{DATASET_DIR}/{subpath}"
    if not os.path.exists(dataset_dir):
        return

    # Make a list of images under the folder
    image_dirs = [int(folder) for folder in os.listdir(dataset_dir) if folder.isdigit()]
    image_dirs.sort()
    if len(image_dirs) <= 0:
        st.write(f"No images available in the {subpath} dataset dir.")
        return None

    # Create an index for navigating through images in the list
    if 'image_index' not in st.session_state:
        st.session_state.image_index = {}
    dataset_key = f"{subpath}"
    if dataset_key not in st.session_state.image_index.keys():
        st.session_state.image_index[dataset_key] = image_dirs[0] if start_index <= 0 else start_index

    # Display Images and Navigation Buttons
    image_index = st.session_state.image_index[dataset_key]
    no_file_path = f"{dataset_dir}/{image_index}/no"
    skip_file_path = f"{dataset_dir}/{image_index}/skip"

    # Navigation buttons
    updated_index = None
    btn1, btn11, btn2, btn3, btn4, btn5, btn51 = st.columns(7)

    with btn1:
        if button("Prev", "ArrowLeft", lambda: None):
            image_index = prev_image(image_dirs, image_index)
    with btn11:
        if button("-20", "Ctrl+ArrowLeft", lambda: None):
            image_index = prev_image(image_dirs, image_index, 20)
    with btn2:
        if button("Yes", "a", lambda: None, hint=True):
            try:
                if os.path.exists(no_file_path):
                    os.unlink(no_file_path)
                if os.path.exists(skip_file_path):
                    os.unlink(skip_file_path)
                updated_index = image_index
            except:
                pass
    with btn3:
        if button("Skip", "s", lambda: None, hint=True):
            try:
                if os.path.exists(no_file_path):
                    os.unlink(no_file_path)
                open(skip_file_path, 'w').close()
                updated_index = image_index
            except:
                pass
    with btn4:
        if button("No", "d", lambda: None, hint=True):
            try:
                if os.path.exists(skip_file_path):
                    os.unlink(skip_file_path)
                open(no_file_path, 'w').close()
                updated_index = image_index
            except:
                pass
    with btn5:
        if button("Next", "ArrowRight", lambda: None):
            image_index = next_image(image_dirs, image_index)
    with btn51:
        if button("+20", "Ctrl+ArrowRight", lambda: None):
            image_index = next_image(image_dirs, image_index, 20)

    st.session_state.image_index[dataset_key] = image_index

    # Display image
    image_path = f"{dataset_dir}/{image_index}/image.jpg"
    try:
        with open(image_path, "rb") as img_file:
            img = Image.open(img_file)
            st.image(img, use_container_width=True)
    except:
        # If image cannot be loaded, display the error one
        load_error_path = os.path.dirname(__file__) + "/load_error.jpg"
        with open(load_error_path, "rb") as img_file:
            img = Image.open(img_file)
            st.image(img, use_container_width=True)

    no_file_path = f"{dataset_dir}/{image_index}/no"
    skip_file_path = f"{dataset_dir}/{image_index}/skip"
    data_json_path = f"{dataset_dir}/{image_index}/data.json"

    if os.path.exists(skip_file_path):
        match_result = "Skip"
    elif os.path.exists(no_file_path):
        match_result = "No"
    else:
        match_result = "Yes"
    col1, col2 = st.columns(2)
    col1.write(f"Image: **{image_index}**")
    col2.write(f"Match: **{match_result}**")

    # Read data.json and display msg if match_result is "Yes"
    match_msg = ""
    if match_result == "Yes" or match_result == "Skip":
        try: 
            with open(data_json_path, "r") as file:
                data = json.load(file)
                if data["msg"] is not None:
                    match_msg = data["msg"]
        except:
            pass
    st.write(f'{match_msg}')

    return updated_index

def add_to_queue(new_item, queue_list):
    # Rename dataset_path folder with timestamp
    new_item_dir = f"{DATASET_DIR}/{new_item}"
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    mv_folder_name = f"{DATASET_DIR}/{new_item}.0.0.{timestamp}"
    remove_matching(f"{DATASET_DIR}/{new_item}.0.*")
    os.rename(new_item_dir, mv_folder_name)

    # Create a new empty dataset_path folder
    os.makedirs(new_item_dir)
    del st.session_state.image_index[new_item]

    # Add the newly renamed folder on top of queue_dir_list list
    queue_list.insert(0, mv_folder_name[len(DATASET_DIR):])

    # Remove the eldest folder in the queue if MAX_QUEUE is reached
    for ii in range(MAX_QUEUE, len(queue_list)):
        rm_path = f"{DATASET_DIR}/{queue_list[ii]}"
        shutil.rmtree(rm_path)

    # Rotate folders listed in queue_dir_list
    for ii in range(MAX_QUEUE - 1, -1, -1):
        if ii >= len(queue_list):
            continue
        cur_item = queue_list[ii]
        cur_dir = f"{DATASET_DIR}/{cur_item}"
        base_name = cur_item.split('.')[0]
        base_dir = f"{DATASET_DIR}/{base_name}"
        try:
            old_index = int(cur_item.split('.')[1])
            new_index = old_index + 1
            last_labeled = cur_item.split('.')[2]
            timestamp = cur_item.split('.')[3]
        except:
            print(f"Skipping invalid dataset queue list item {cur_item}, expected channel/object.N.M.TIMESTAMP")
            continue
        if old_index <= ii:
            new_dir = f"{base_dir}.{new_index}.{last_labeled}.{timestamp}"
            os.rename(cur_dir, new_dir)

def count_images(dataset_path):
    num_images = 0
    num_skip = 0
    num_no = 0
    for folder in os.listdir(dataset_path):
        if not folder.isdigit(): continue
        num_images += 1
        if os.path.exists(f"{dataset_path}/{folder}/skip"):
            num_skip += 1
            continue
        if os.path.exists(f"{dataset_path}/{folder}/no"):
            num_no += 1
            continue
    num_yes = num_images - num_no - num_skip
    return num_images, num_skip, num_no, num_yes

# Dataset management state machine section
def dataset_management_sm(key):
    hide_shortcut_iframe()
    st.header("Data Collection")

    # Load data from JSON files
    sources_data = load_data(imgsrc_cfg_json_path)
    objects_data = load_data(objects_cfg_json_path)
    # Extract channel ids and object ids
    channels, objects = extract_ids(sources_data, objects_data)
    # Generate all permutations of channels and objects
    data_combinations = generate_permutations(channels, objects)

    # Filter valid combinations
    valid_combinations = [comb for comb in data_combinations if os.path.exists(f"{DATASET_DIR}/{comb[0]}/{comb[1]}")]

    # Add UI that displays the combinations to select
    st.markdown("### **Select a channel/object combination:**")
    selected_combination = st.selectbox("channel/object", label_visibility='collapsed', options=[f"{comb[0]}/{comb[1]}" for comb in valid_combinations])

    # Show user the number of images in the dataset, number of dataset queued for labeling and
    # a button allowing to add the currently selected dataset to the queue.
    dataset_path = f"{DATASET_DIR}/{selected_combination}"
    num_images, num_skip, num_no, num_yes = count_images(dataset_path)
    
    queue_list = []
    queue_nums = 0
    for ii in range (1,MAX_QUEUE + 1):
        dir_pattern = f"{dataset_path}.{ii}.*"
        matching_subpaths = [directory[len(DATASET_DIR) + 1:] for directory in glob.glob(dir_pattern) if os.path.isdir(directory)]
        if len(matching_subpaths) > 0:
            queue_nums += 1
        queue_list.extend(matching_subpaths)

    st.text(f"Number of images in the working dataset: {num_images} (skip:{num_skip} no:{num_no} yes:{num_yes})\n" +
            f"Datasets in labeling queue: {queue_nums} (out of {MAX_QUEUE})")

    # Add a button for adding the selected combination to the queue
    if st.button("Add to queue"):
        add_to_queue(selected_combination, queue_list)
        st.rerun()

    st.markdown("### **Browse captured images:**")
    image_browsing(selected_combination)

    # Add a "Back" button to go back to the start form
    if st.button("Back"):
        st.session_state.app_state = "init"
        st.rerun()

# get the verbal description of the channel specified by id
def get_channel_name(chanid):
    c_desc = None
    try:
        with open(imgsrc_cfg_json_path, 'r') as f:
            data = json.load(f)
        for channel in data[CFG_channels_key]:
            if channel[CFG_chan_id_key] == chanid:
                c_desc = channel[CFG_chan_name_key]
    except:
        pass
    return c_desc

# get the verbal description of the object specified by id
def get_object_description(objid):
    obj_desc = None
    try:
        with open(objects_cfg_json_path, 'r') as f:
            data = json.load(f)
        for object in data['objects']:
            if object[CFG_obj_id_key] == objid:
                obj_desc = object[CFG_obj_desc_key]
    except:
        pass
    return obj_desc

# Function for adding a row to the training dataset dataframe
def add_train_data_row(dataframe, dir, timestamp, c_desc, o_desc, location, res):
    try:
        image_pname = f"{dir}/{IMG_file_name}"
        img_data = base64.b64encode(Path(image_pname).read_bytes()).decode()
        dataframe.loc[len(dataframe)] = {
            "ver": DTS_version,
            "tstamp": timestamp,
            "img": img_data,
            "c_desc": c_desc,
            "o_desc": o_desc,
            "location": location,
            "res": res,
        }
    except Exception as e:
        print(f"Skipping {image_pname} due to error: {e}")
        return

def move_to_train_data_file(selected_dataset):
    try:
        dataset_dir = f"{DATASET_DIR}/{selected_dataset}"
        chan_obj = selected_dataset.split('.')[0]
        last_labeled = int(selected_dataset.split('.')[2])
        timestamp = selected_dataset.split('.')[3]
        chan, obj = chan_obj.split('/')
    except:
        return f"Invalid format for the {selected_dataset}, expected channel/object.N.M.TIMESTAMP"

    num_images = len([folder for folder in os.listdir(dataset_dir) if folder.isdigit()])
    try: allow_no_label = st.session_state['dataset_allow_no_label']
    except: allow_no_label = False
    if last_labeled < num_images and not allow_no_label:
        return f"Labeled only {last_labeled} of {num_images} images!"

    # Let's find relevant info for channel and object in the config
    # Note: it could be different from what was captured when inference was run
    c_desc = get_channel_name(chan)
    if c_desc is None:
        return f"Unable to find the name of the channel \"{chan}\" in the system configuration!"
    o_desc = get_object_description(obj)
    if o_desc is None:
        return f"Unable to find the description of the object \"{obj}\" in the system configuration!"

    # Load the picke dataframe or create a new one
    columns = ["ver", "tstamp", "img", "c_desc", "o_desc", "location", "res"]
    if os.path.exists(pcl_train_data_path):
        try:
            df = pd.read_pickle(pcl_train_data_path)
            # Add missing columns
            for column in columns:
                if column not in df.columns:
                    df[column] = None
            # Drop extra columns
            extra_cols = [col for col in df.columns if col not in columns]
            df.drop(extra_cols, axis=1, inplace=True)
        except Exception as e:
            return f"Error loading data: {str(e)}"
    else:
        df = pd.DataFrame(columns=columns)

    # Check if timestamp already exists in dataframe
    try: allow_override = st.session_state['dataset_allow_overide']
    except: allow_override = False
    if timestamp in df['tstamp'].values:
        if not allow_override:
            return f"Error: Data with timestamp {timestamp} already exists!"
        else:
            df = df[df['tstamp'] != timestamp]
            df.reset_index(drop=True, inplace=True)

    # Append the information from the dataset
    subdirs = [f.path for f in os.scandir(dataset_dir) if f.is_dir()]
    for s in subdirs:
        if os.path.exists(f"{s}/skip"):
            continue
        if os.path.exists(f"{s}/no"):
            add_train_data_row(df, f"{s}", timestamp, c_desc, o_desc, None, "No")
        else:
            location = None
            try:
                with open(f"{s}/location", "r") as location_file:
                    location = location_file.read()
            except: pass # no location description
            add_train_data_row(df, f"{s}", timestamp, c_desc, o_desc, location, "Yes")

    try:
        if os.path.exists(pcl_train_data_path):
            os.rename(pcl_train_data_path, pcl_train_data_path + ".bak")
        df.to_pickle(pcl_train_data_path)
    except Exception as e:
        return f"Error saving {pcl_train_data_path}: {str(e)}"
    
    return None

# Automatically label an image, image_path - image folder path
# skip_if_correct - mark the image to be skipped if the label is already correct
# do_location - create file "location" with the object location descripton for the positive samples
def label_image(image_path, skip_if_correct, do_location):
    print(f"Labeling image: {image_path}...", end="")
    image_pname = f"{image_path}/image.jpg"
    data_json_path = f"{image_path}/data.json"
    location_file_path = f"{image_path}/location"
    skip_file_path = f"{image_path}/skip"
    no_file_path = f"{image_path}/no"
    if os.path.exists(skip_file_path):
        print(f"ignoring, already marked as skip")
        return
    try: 
        with open(data_json_path, "r") as file:
            data = json.load(file)
            c_name = data["c_name"]
            o_desc = data["o_desc"]
    except:
        # the old style data.json did not have o_desc, will not support those datasets
        try: open(skip_file_path, 'a').close(); print(f"invalid image data.json, skipping")
        except: print("invalid image data.json, error marking to skip")
        return
    img_data = base64.b64encode(Path(image_pname).read_bytes()).decode()
    res, msg = MODEL_INTERFACE.locate(img_data, o_desc, c_name, do_location)
    # if msg is None then something did work in the model interface, giving up
    if msg is None:
        print(f"ignoring, no response from the model")
        return
    # record the location if requested and the model response msg is not empty
    if do_location and len(msg) > 0:
        try:
            with open(location_file_path, "w") as location_file:
                location_file.write(msg)
        except: pass
    # positive, but the current result is negative
    if res and os.path.exists(no_file_path):
        try: os.unlink(no_file_path); print("corrected to positive")
        except: print("error correcting to positive")
    # negative, but the current result is positive
    elif not res and not os.path.exists(no_file_path):
        try: open(no_file_path, 'a').close(); print("corrected to negative")
        except: print("error correcting to negative")
    # current and auto-labeling results match
    elif skip_if_correct:
        try: open(skip_file_path, 'a').close(); print("correct, marked to skip")
        except: print("correct, error marking to skip")
    else:
        print("correct")

# Create a UI section for auto-labeling with a progress bar
# dataset_dir - directory with image subdirs (the names are integers)
# num_images - number of images (subdirs) in the dataset
def auto_labeling(dataset_dir, num_images):
    skip_if_correct = st.session_state['auto_label_skip_if_correct']
    do_location = st.session_state['auto_label_do_location']
    with st.spinner("Labeling images..."):
        progress_bar = st.progress(0)
        for i, folder in enumerate(os.listdir(dataset_dir)):
            if not folder.isdigit(): continue
            image_dir = os.path.join(dataset_dir, folder)
            label_image(image_dir, skip_if_correct, do_location)
            progress_bar.progress((i + 1) / num_images)
    st.success("All images have been labeled!")

# Dataset labeling state machine section
def dataset_labeling_sm(key):
    hide_shortcut_iframe()
    st.header("Labeling Collected Data")

    # Load data from JSON files
    sources_data = load_data(imgsrc_cfg_json_path)
    objects_data = load_data(objects_cfg_json_path)
    # Extract channel ids and object ids
    channels, objects = extract_ids(sources_data, objects_data)
    # Generate all permutations of channels and objects
    data_combinations = generate_permutations(channels, objects)

    # Filter valid combinations
    valid_combinations = [comb for comb in data_combinations if os.path.exists(f"{DATASET_DIR}/{comb[0]}/{comb[1]}")]

    # Add UI that displays the combinations for labeling
    st.markdown("### **Select a channel/object combination:**")
    selected_combination = st.selectbox("channel/object/2", label_visibility='collapsed', options=[f"{comb[0]}/{comb[1]}" for comb in valid_combinations])

    # Let user select one of the datasets queued for labeling here (for the selected channel/object combination)
    datasets_queue_path = f"{DATASET_DIR}/{selected_combination}"
    queue_list = []
    queue_nums = 0
    for ii in range (1,MAX_QUEUE + 1):
        dir_pattern = f"{datasets_queue_path}.{ii}.*.*"
        matching_subpaths = [directory[len(DATASET_DIR) + 1:] for directory in glob.glob(dir_pattern) if os.path.isdir(directory)]
        if len(matching_subpaths) > 0:
            queue_nums += 1
        queue_list.extend(matching_subpaths)

    if len(queue_list) > 0:
        # Select box to display datasets in the queue
        st.markdown("### **and a dataset from the training queue:**")
        if f"{selected_combination}_dataset_selection" in st.session_state.keys():
            st.session_state["dataset_selection"] = st.session_state[f"{selected_combination}_dataset_selection"]

        col1, col2 = st.columns([3, 1])  # Create two columns with a ratio of 3:1
        selected_dataset = col1.selectbox("dataset", label_visibility='collapsed', options=queue_list, key = "dataset_selection")
        dataset_dir = f"{DATASET_DIR}/{selected_dataset}"
        if col2.button('Move to TrainSet'):
            err = move_to_train_data_file(selected_dataset)
            if err is not None:
                st.error(err)
            else:
                try:
                    name_split = selected_dataset.split('.')
                    name_split.insert(1, 'del')
                    del_name = '.'.join(name_split)
                    os.rename(dataset_dir, f"{DATASET_DIR}/{del_name}")
                except Exception as e:
                    st.error(str(e))
                else:
                    st.rerun()

        col21, col22 = st.columns([1, 1])
        col21.toggle(label="Allow override", value=False, key='dataset_allow_overide')
        col22.toggle(label="Allow incomplete labeling", value=False, key='dataset_allow_no_label')

        num_images, num_skip, num_no, num_yes = count_images(dataset_dir)
        st.text(f"Number of images in the selected dataset: {num_images} (skip:{num_skip} no:{num_no} yes:{num_yes})")

        # Add the "Label Automatically" user elements
        col31, col32, col33 = st.columns(3)
        auto_label_button = col31.button("Label Automatically", key='auto_label_button')
        col32.toggle("Skip if correct", value=True, key='auto_label_skip_if_correct')
        col33.toggle("Describe location", value=True, key='auto_label_do_location')
   
        if auto_label_button:
            auto_labeling(dataset_dir, num_images)

        try:
            chan_obj = selected_dataset.split('.')[0]
            dataset_index = int(selected_dataset.split('.')[1])

            last_labeled = int(selected_dataset.split('.')[2])
            timestamp = selected_dataset.split('.')[3]

            updated_index = image_browsing(selected_dataset, start_index = last_labeled)
            if updated_index is not None and updated_index > last_labeled:
                new_selection = f"{chan_obj}.{dataset_index}.{updated_index}.{timestamp}"
                new_dir_name = f"{DATASET_DIR}/{new_selection}"
                try:
                    os.rename(dataset_dir, new_dir_name)
                    last_labeled = updated_index
                    dataset_dir = new_dir_name
                    st.session_state[f"{chan_obj}_dataset_selection"] = f"{new_selection}"
                except Exception as e:
                    print("Exception: ", str(e))
                    pass
            else:
                if f"{chan_obj}_dataset_selection" in st.session_state.keys():
                    del(st.session_state[f"{chan_obj}_dataset_selection"])
        except Exception as e:
            print(f"Invalid dataset queue list item {selected_dataset}, expected channel/object.N.M.TIMESTAMP")
            print("Exception: ", str(e))

        st.markdown("<div style=\"font-family: monospace; font-size: 12px; white-space: pre;\">\n"
                    "The dataset folder naming pattern is <b>channel/object.N.M.TIMESTAMP</b>, where:\n" +
                f"<b>N</b> - the dataset folder position in the queue, when a new data folder is added to the queue, the\n" +
                f"    positions shift, and one folder from the tail is removed if the queue length is exceeding <b>{MAX_QUEUE}</b>.\n" +
                f"<b>M</b> - the higest index of the labeled image in the dataset folder, an image is considered\n" +
                f"    to be labeled if \"Yes\", \"No\" or \"Skip\" buttin is pressed for it.\n" +
                f"<b>TIMESTAMP</b> - time when the dataset folder was added to the queue." +
                    "</div><div><br/></div>", unsafe_allow_html=True)

    # Add a "Back" button to go back to the start form
    if st.button("Back"):
        st.session_state.app_state = "init"
        st.rerun()

