version: '3.8'

services:
  orchestrator:
    image: orchestrator-image
    container_name: orchestrator-container
    volumes:
      - ./images:/images
      - ./sysconfig:/sysconfig
      - ./events:/events
    depends_on:
      - ui
      - alexa
      - imager
      #- llm

  # Ollama can be used locally or in docker
  # Under Linux it's esier to install locally: 
  #  curl -fsSL https://ollama.com/install.sh | sh
  # then use the below command to load and manually try the model
  #  ollama run llama3.2-vision:11b-instruct-fp16
  # (can try "llama3.2-vision:latest" for 4-bit quantized version)
  #
  # For docker setup see https://hub.docker.com/r/ollama/ollama
  #  llm:
  #    image: docker pull ollama/ollama:0.5.1
  #    expose:
  #     - 11434/tcp
  #    healthcheck:
  #      test: ollama --version || exit 1
  #    command: serve
  #    restart: on-failure
  #    volumes:
  #      - ${OLLAMA_MODELS_DIR:-./ollama}:/root/.ollama
  #      - ${DATA_DIR:-.}/images:/images
  #      - ${DATA_DIR:-.}/sysconfig:/sysconfig
  #      - ${DATA_DIR:-.}/events:/events
  #    deploy:
  #      resources:
  #        reservations:
  #          devices:
  #            - driver: nvidia
  #              device_ids: ['all']
  #              capabilities: [gpu]

  alexa:
    image: alexa-image
    container_name: alexa-container
    volumes:
      - ${DATA_DIR:-.}/images:/images
      - ${DATA_DIR:-.}/sysconfig:/sysconfig
      - ${DATA_DIR:-.}/events:/events

  imager:
    image: imager-image
    container_name: imager-container
    volumes:
      - ${DATA_DIR:-.}/images:/images
      - ${DATA_DIR:-.}/sysconfig:/sysconfig
      - ${DATA_DIR:-.}/events:/events

  ui:
    image: ui-image
    container_name: ui-container
    ports:
      - "8501:8501"
    volumes:
      - ${DATA_DIR:-.}/images:/images
      - ${DATA_DIR:-.}/sysconfig:/sysconfig
      - ${DATA_DIR:-.}/events:/events

