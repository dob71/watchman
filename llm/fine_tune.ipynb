{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fine tuning LLAMA 3.2 Vision</h1>\n",
    "This notebook uses the Watchman dataset data to fine tune the LLAMA 3.2 Vision Instruct model</br>\n",
    "and deploy the fine tuned version to be served by vLLM.</br>\n",
    "It uses Unsloth for fine tuning. In order to fit to single 24G VRAM GPU (no free multi-GPU support in Unsloth now)</br>\n",
    "it loads 4-bit quatized base model weights. After fine-tuning, when merging the LoRA adapters and exporting the</br>\n",
    "resulted weights are converted to 16bit. At the moment this appears to be the only easily available option.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Will be doing plain LoRA fine tuning, nothing fancy to start with.\n",
    "- Unsloth looks like the easiest path forward:</br>\n",
    "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb\n",
    "- Apparently, Unsloth will only work on 1 GPU for free... Still worth a try w/ the 4bit quantized model.\n",
    "- Will try fine tuning for the vision layers only first.\n",
    "- Installed it using \"pip3 install unsloth\" (looks like it updated torch version..., everything still works fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unsloth import FastVisionModel\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel\n",
    "from unsloth import is_bf16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from openai import OpenAI\n",
    "from peft import PeftModelForCausalLM\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "model = None\n",
    "processor = None\n",
    "last_loras_dir = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location where to get LoRA addapters from the last picked training.\n",
    "# Do not run the cell if want to train from scratch.\n",
    "#last_loras_dir = f\"./trained/watchman_model_last_loras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    last_loras_dir if last_loras_dir is not None else \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(model, PeftModelForCausalLM):\n",
    "    model = FastVisionModel.get_peft_model(\n",
    "        model,\n",
    "        finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "        finetune_language_layers   = False, # False if not finetuning language layers\n",
    "        finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "        finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "        r = 24,           # The larger, the higher the accuracy, but might overfit\n",
    "        lora_alpha = 32,  # Recommended alpha == r at least\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        random_state = 42,\n",
    "        use_rslora = False,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    "        # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model interfaces we can use for testing\n",
    "print(f\"Working dir: {os.getcwd()}\")\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../orchestrator\"))\n",
    "from shared_settings import *\n",
    "from model_interfaces import *\n",
    "print(MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model_name interface (used only to get the prompt from the Watchman code)\n",
    "model_name = \"vllm-complex\"\n",
    "MODEL_INTERFACE = MODELS[model_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas dataframe created and saved by the UI.\n",
    "# See ui/dataset.py file fo the details and the description of columns.\n",
    "DATA_DIR = '../.data' # point to your Watchman data directory\n",
    "pickle_file = f\"{DATA_DIR}/{CFG_dset_svc_name}/{DTS_train_data_file}\"\n",
    "df = pd.read_pickle(pickle_file)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the number of samples, the \"no_to_yes_ratio\" can be used to balance it unevenly\n",
    "# and have the model prefer the \"no\" if the ratio is above 1.0 or \"yes\", if it's below.\n",
    "def balance_dataset(df, target_column='res', no_to_yes_ratio=1.1):\n",
    "    # Separate the classes\n",
    "    df_yes = df[df[target_column] == 'Yes']\n",
    "    df_no = df[df[target_column] == 'No']\n",
    "    \n",
    "    # Determine which class has to be reduced.\n",
    "    # There are a lot of images, it rmoves the excess rather than creating duplicate rows.\n",
    "    num_yes = len(df_yes)\n",
    "    num_no = len(df_no)\n",
    "    cur_ratio = num_no / num_yes\n",
    "    if cur_ratio > no_to_yes_ratio:\n",
    "        need_no_samples = int(num_yes * no_to_yes_ratio) + 1\n",
    "        df_no = resample(df_no, replace=False, n_samples=need_no_samples, random_state=42)\n",
    "    else:\n",
    "        need_yes_samples = int(num_no / no_to_yes_ratio) + 1\n",
    "        df_yes = resample(df_yes, replace=False, n_samples=need_yes_samples, random_state=42)\n",
    "    \n",
    "    # Combine the balanced datasets\n",
    "    return pd.concat([df_yes, df_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets from the data we have available\n",
    "df_train, df_test = train_test_split(balance_dataset(df), test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for the Watchman data\n",
    "class WatchmanDataset(Dataset):\n",
    "    def __init__(self, df, WATCHMAN_MODEL_INTERFACE):\n",
    "        self.df = df\n",
    "        self.MI = WATCHMAN_MODEL_INTERFACE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_data = row['img']\n",
    "        c_desc = row['c_desc']\n",
    "        o_desc = row['o_desc']\n",
    "        res = row['res']  # Correct completion without <|end_of_text|>\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = self.MI.gen_detect_prompt(o_desc, c_desc)\n",
    "\n",
    "        # Load the image\n",
    "        image = self.load_image_from_base64(img_data)\n",
    "\n",
    "        # Prepare completion\n",
    "        # The untrained model answers with \"\\n\\nYes.<eos>\" or \"\\n\\nNo.<eos>\", so add the\n",
    "        # completion for the expected result matching the original as much as possible.\n",
    "        completion = \"\\n\\n\" + res + '.<|end_of_text|>'\n",
    "\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"res\": completion,\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "    def load_image_from_base64(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatchmanDataCollator(UnslothVisionDataCollator):\n",
    "    def __init__(self, model, processor, prompt_end_text = \"<|start_header_id|>assistant<|end_header_id|>\"):\n",
    "        super().__init__(model, processor)\n",
    "        self.text_tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n",
    "        self.prompt_end = self.text_tokenizer(prompt_end_text, add_special_tokens=False)['input_ids']\n",
    "        self.prompt_end_text = prompt_end_text\n",
    "\n",
    "    def find_end_of_prompt(self, token_list):\n",
    "        len_seq = len(self.prompt_end)\n",
    "        for i in range(len(token_list) - len_seq + 1):\n",
    "            if token_list[i:i+len_seq] == self.prompt_end:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        full_sequences = []\n",
    "        images = []\n",
    "        \n",
    "        for example in examples:    \n",
    "            prompt = example[\"prompt\"]\n",
    "            res = example[\"res\"]\n",
    "            image = example[\"image\"]\n",
    "            full_sequences.append(prompt + res)\n",
    "            images.append(image)\n",
    "\n",
    "        # Tokenize the texts and process the images\n",
    "        batch = self.processor(\n",
    "            text    = full_sequences,\n",
    "            images  = images,\n",
    "            padding = True,\n",
    "            add_special_tokens = False,\n",
    "            return_tensors = \"pt\",\n",
    "        )\n",
    "        batch.pop(\"token_type_ids\", None)\n",
    "\n",
    "        # Original unsloth collator accepts multiple images,\n",
    "        # shouldn't matter for the watchman data (single image)\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        if type(pixel_values) is list:\n",
    "            for j, pixel_value_j in enumerate(pixel_values):\n",
    "                if type(pixel_value_j) is list:\n",
    "                    for k, pixel_value_k in enumerate(pixel_value_j):\n",
    "                        pixel_value_j[k] = pixel_value_k.to(self.dtype)\n",
    "                else:\n",
    "                    pixel_values[j] = pixel_value_j.to(self.dtype)\n",
    "            pass\n",
    "            batch[\"pixel_values\"] = pixel_values\n",
    "        else:\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(self.dtype)\n",
    "        pass\n",
    "\n",
    "        # Mask prompt and pad tokens before returning the batch\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        for i, l in enumerate(labels):\n",
    "            last_match_idx = self.find_end_of_prompt(l.tolist())\n",
    "            if last_match_idx < 0:\n",
    "                raise ValueError(f\"Invalid prompt,the prompt must end with: {self.prompt_end_text}\")\n",
    "            labels[i, :last_match_idx] = self.ignore_index\n",
    "        labels[torch.isin(labels, self.padding_token_ids)] = self.ignore_index\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test instances of our custom dataset\n",
    "if df_test[df_test['res'] == 'No'].shape[0] < 0:\n",
    "    print(\"Can't find any No samples in the df_test\")\n",
    "elif df_test[df_test['res'] == 'Yes'].shape[0] < 0:\n",
    "    print(\"Can't find any Yes samples in the df_test\")\n",
    "elif df_train[df_train['res'] == 'No'].shape[0] < 0:\n",
    "    print(\"Can't find any No samples in the df_train\")\n",
    "elif df_train[df_train['res'] == 'Yes'].shape[0] < 0:\n",
    "    print(\"Can't find any Yes samples in the df_train\")\n",
    "else:\n",
    "    d_train = WatchmanDataset(df_train, MODEL_INTERFACE)\n",
    "    d_test = WatchmanDataset(df_test, MODEL_INTERFACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable model for inference\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# Test that we can use the dataset for inference\n",
    "def find_samples(dataset):\n",
    "    found_yes = False\n",
    "    found_no = False\n",
    "    yes_sample = None\n",
    "    no_sample = None\n",
    "\n",
    "    while not (found_yes and found_no):\n",
    "        sample = dataset[torch.randint(0, len(dataset), (1,)).item()]\n",
    "        if 'yes' in sample['res'].lower() and not found_yes:\n",
    "            yes_sample = sample\n",
    "            found_yes = True\n",
    "        elif 'no' in sample['res'].lower() and not found_no:\n",
    "            no_sample = sample\n",
    "            found_no = True\n",
    "\n",
    "    return yes_sample, no_sample\n",
    "\n",
    "# Process the samples\n",
    "yes_sample, no_sample = find_samples(d_train)\n",
    "\n",
    "for sample in [yes_sample, no_sample]:\n",
    "    inputs = processor(sample['image'], sample['prompt'], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=200,\n",
    "        temperature=None,\n",
    "        min_p=None,\n",
    "        do_sample=False\n",
    "    )\n",
    "    # Decode output\n",
    "    rsp_prefix = processor.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    img = sample['image']\n",
    "    w, h = img.size\n",
    "    display(img.resize((int(w / 4), int(h / 4))))\n",
    "    print(\"Inference result: \", response[len(rsp_prefix):])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all samples in the dataset and build the truth matrix\n",
    "def process_dataset(dataset, model, processor):\n",
    "    truth_matrix = []\n",
    "    d_len = len(dataset)\n",
    "    count = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        inputs = processor(\n",
    "            sample['image'], \n",
    "            sample['prompt'], \n",
    "            add_special_tokens=False, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            use_cache=True,\n",
    "            max_new_tokens=200,\n",
    "            temperature=None,\n",
    "            min_p=None,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        rsp_prefix = processor.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "        response = processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Determine if the response matches 'yes' or 'no'\n",
    "        real_answer = sample['res'].lower()\n",
    "        correct_answer = response[len(rsp_prefix):].lower()\n",
    "        truth = (\"yes\" in correct_answer) ^ (\"no\" in real_answer)\n",
    "        \n",
    "        # Append result to truth matrix\n",
    "        truth_matrix.append({\n",
    "            'sample': sample,\n",
    "            'truth': truth,\n",
    "            'response': correct_answer\n",
    "        })\n",
    "        count += 1\n",
    "        print(f\"\\rProcessing {count}/{d_len}\", end=\"\")        \n",
    "\n",
    "    print(\" done\")\n",
    "    return truth_matrix\n",
    "\n",
    "# Visualize the truth matrix from process_dataset()\n",
    "def create_confusion_matrix(truth_matrix):\n",
    "    # Count all categories\n",
    "    FN = sum(1 for item in truth_matrix if 'yes' in item['sample']['res'].lower() and not item['truth'])\n",
    "    TP = sum(1 for item in truth_matrix if 'yes' in item['sample']['res'].lower() and item['truth'])\n",
    "    FP = sum(1 for item in truth_matrix if 'no' in item['sample']['res'].lower() and not item['truth'])\n",
    "    TN = sum(1 for item in truth_matrix if 'no' in item['sample']['res'].lower() and item['truth'])\n",
    "\n",
    "    # Construct the 2x2 matrix\n",
    "    cm = np.array([\n",
    "        [FN, TN],\n",
    "        [TP, FP],\n",
    "    ])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    # Custom labels for Yes/No\n",
    "    classes = ['Yes', 'No']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    classes.reverse()\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2. if cm.max() else 1  # Avoid division by zero if all counts are 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "            # Add labels for clarity\n",
    "            if i == 0 and j == 0:\n",
    "                plt.text(j, i + 0.3, 'False Negative', ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "            elif i == 0 and j == 1:\n",
    "                plt.text(j, i + 0.3, 'True Negative', ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "            elif i == 1 and j == 0:\n",
    "                plt.text(j, i + 0.3, 'True Positive', ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "            else:\n",
    "                plt.text(j, i + 0.3, 'False Positive', ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('True label')\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable model for inference and do full stats on inference over the test dataset,\n",
    "# (i.e. for calculating the confusion matrix to see how it performs before training)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = DataParallel(model)\n",
    "#    model = model.to(torch.device('cuda'))\n",
    "# The inference should work w/ multiple GPUs, TBD.\n",
    "FastVisionModel.for_inference(model)\n",
    "truth_matrix = process_dataset(d_test, model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(truth_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for training data\n",
    "train_data_dir = './trained'\n",
    "os.makedirs(train_data_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup (in case need to retry the trainer setup)\n",
    "trainer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = processor,\n",
    "    data_collator = WatchmanDataCollator(model, processor),\n",
    "    train_dataset = d_train,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 10,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 0,\n",
    "        # max_steps = 10, # Use to try it out (still quite slow)\n",
    "        num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = f\"{train_data_dir}/outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable for training and do a train run saving the snapshot after each epoch\n",
    "epoches_to_train = 5 # 5 should be enough\n",
    "FastVisionModel.for_training(model)\n",
    "trainer_stats = []\n",
    "for count in range(1, epoches_to_train + 1):\n",
    "    stats = trainer.train()\n",
    "    trainer_stats.append(stats)\n",
    "    print(f\"Saving LoRA adapters epoch {count}...\")\n",
    "    model.save_pretrained(f\"{train_data_dir}/watchman_model{count}\")\n",
    "    processor.save_pretrained(f\"{train_data_dir}/watchman_model{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable model for inference and see how we are doing now with our two test samples.\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "for sample in [yes_sample, no_sample]:\n",
    "    inputs = processor(sample['image'], sample['prompt'], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        max_new_tokens=200,\n",
    "        temperature=None,\n",
    "        min_p=None,\n",
    "        do_sample=False\n",
    "    )\n",
    "    # Decode output\n",
    "    rsp_prefix = processor.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    img = sample['image']\n",
    "    w, h = img.size\n",
    "    display(img.resize((int(w / 4), int(h / 4))))\n",
    "    print(\"Inference result: \", response[len(rsp_prefix):])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_saved_model(saved_model_num):\n",
    "    dir = f\"{train_data_dir}/watchman_model{saved_model_num}\"\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name = dir,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastVisionModel.for_inference(model)\n",
    "    truth_matrix = process_dataset(d_test, model, processor)\n",
    "    create_confusion_matrix(truth_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup (in case need to retry the trainer setup)\n",
    "trainer = None\n",
    "model = None\n",
    "processor = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the fine tuned models to see how they perform after each epoch.</br>\n",
    "Basing on the results will pick one to merge the LoRA adapters and convert the weights</br>\n",
    "to a compatible for serving format, then deploy for real use evaluation.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_saved_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_saved_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_saved_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_saved_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_saved_model(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the chances of overfitting pick the first epoch where the results ar satisfactory.</br>\n",
    "I.e. the first one that made it without or with minimum errors, especially in false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_num = 4 # change the number for the best snapshot you picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selected snapshot and check if it still makes any mistakes on the train data\n",
    "dir = f\"{train_data_dir}/watchman_model{saved_model_num}\"\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    model_name = dir,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "truth_matrix = process_dataset(d_train, model, processor)\n",
    "create_confusion_matrix(truth_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show failing samples from the truth_matrix\n",
    "def show_mismatch(truth_matrix, ans, max=2):\n",
    "    count = 0\n",
    "    for item in truth_matrix:\n",
    "        if count >= max:\n",
    "            break\n",
    "        sample = item['sample']\n",
    "        model_ans = sample['res'].lower()\n",
    "        if item['truth']:\n",
    "            continue\n",
    "        if not ans in model_ans:\n",
    "            continue\n",
    "        img = sample['image']\n",
    "        w, h = img.size\n",
    "        display(img.resize((int(w / 2), int(h / 2))))\n",
    "        print(f\"Model response: {item['response']}\")\n",
    "        print(\"---------------------------\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a couple of false negatives (if any)\n",
    "show_mismatch(truth_matrix, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a couple of false positives (if any)\n",
    "show_mismatch(truth_matrix, 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LoRA adapters from the selected pass\n",
    "dir = f\"{train_data_dir}/watchman_model{saved_model_num}\"\n",
    "save_dir = f\"{train_data_dir}/watchman_model_last_loras\"\n",
    "shutil.copytree(dir, save_dir, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally to safetensors 16bit (this should work w/ VLLM).\n",
    "# Converting the merged model to GGUF or converting just LoRA adapters\n",
    "# for serving by OLLAMA is not working/implemented for mllama/unsloth yet.\n",
    "model.save_pretrained_merged(f\"{train_data_dir}/watchman_sft_16bit\", processor, )\n",
    "\n",
    "# Savig to GGUF is not supported for the Vision models in Unsloth at the momnet.\n",
    "# Save to 8bit Q8_0\n",
    "# model.save_pretrained_gguf(\"model\", processor)\n",
    "# Save to 16bit GGUF\n",
    "# model.save_pretrained_gguf(\"model\", processor, quantization_method = \"f16\")\n",
    "# Save to q4_k_m GGUF\n",
    "# model.save_pretrained_gguf(f\"{train_data_dir}/watchman_gguf_q4_k_m\", processor, quantization_method = \"q4_k_m\")\n",
    "# Push to hf:\n",
    "# model.push_to_hub_gguf(\"user/model\", processor, quantization_method = \"f16\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we save merged model in f16 format, there's no qunization anymore.\n",
    "# It has to be removed from the model's config.json if still there (unsloth bug as of now).\n",
    "import os\n",
    "import json\n",
    "def remove_quantization_from_config_json(config_path):\n",
    "    # Load JSON file\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Remove 'quantization_config' if it exists\n",
    "    config.pop(\"quantization_config\", None)\n",
    "\n",
    "    # Save back the modified JSON\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "remove_quantization_from_config_json(f\"{train_data_dir}/watchman_sft_16bit/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup as we now going to try serving the model with vLLM\n",
    "trainer = None\n",
    "model = None\n",
    "processor = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up vLLM (outside of this notebook environment). We'll have it serve the fine-tuned model on the system w/ 2 RTX3090 cards.\n",
    "# In order to bypass the chat templates mess, we will use .jinja file that passes Watchman prompt directly for messages w/ role set to \"watchman\".\n",
    "# If using just one card with enough memory, remove the \"--tensor-parallel-size 2\" option. If out of memory, try reducing \"--max_model_len 2048\" and/or \"--max-num-seqs 16\".\n",
    "# Generate the command to start serving the local fine tuned model:\n",
    "print(f\"vllm serve {os.path.abspath(train_data_dir)}/watchman_sft_16bit --chat-template {os.path.abspath('.')}/watchman_sft_16bit_chat_tpl.jinja --tensor-parallel-size 2 --enforce-eager --gpu-memory-utilization 0.9 --max_model_len 2048 --max-num-seqs 16 --port 5050\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the above commans, give it a minute or two to load everything, and check if we can poke the server.\n",
    "# Note: VRAM cleanup above does not always work, check w/ nvtop and restart the python kernel if it still uses VRAM.\n",
    "!curl http://127.0.0.1:5050/version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's confirm that the model works as expected when seved by vLLM.</br>\n",
    "This is done in the vllm-fine_tune-test.ipnb notebook.</br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
