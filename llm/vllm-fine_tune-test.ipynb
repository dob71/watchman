{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing Fine-Tuned LLAMA 3.2 Vision Served in vLLM</h1>\n",
    "This notebook uses the Watchman dataset data. It runs inference using the fine-tuned model</br>\n",
    "created and deployed with vLLM in the fine_tune.ipynb notebook.</br>\n",
    "This notebook will walk through the whole dataset and use Watchman's model_interfaces.py's</br>\n",
    "'vllm-complex' interface to run inference, then summarize the results to see how we are doing now.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can poke the vLLM server\n",
    "!curl http://127.0.0.1:5050/version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm that vLLM is working and get the id of model served.\n",
    "# Expecting that it's only serving our fine tuned model, otherwise\n",
    "# we will use the last one in the returned model list.\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"NONE_NEEDED\"\n",
    "openai_api_base = \"http://localhost:5050/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(f\"Served models:\")\n",
    "for m in models:\n",
    "    print(f\"  {m.id}\")\n",
    "vllm_model = m.id\n",
    "print(f\"Will use: {vllm_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model interfaces we can use for testing\n",
    "print(f\"Working dir: {os.getcwd()}\")\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../orchestrator\"))\n",
    "from shared_settings import *\n",
    "from model_interfaces import *\n",
    "print(MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vars for finding train data\n",
    "dataset = \"../.data/dataset\" # dataset folder location\n",
    "chans = [\"porch\"] # list of channels to load the data for\n",
    "objs = [\"person\"] # list of objects to load the data for\n",
    "model_name = \"vllm-complex\" # model interface name to use for inference experimentation\n",
    "c_desc = {\n",
    "    \"porch\": \"Porch\",\n",
    "}\n",
    "o_desc = {\n",
    "    \"person\": \"a person\",\n",
    "}\n",
    "MODEL_INTERFACE = MODELS[model_name](model_to_use=vllm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test inference\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import base64\n",
    "\n",
    "def test_inf(s, c, o, res):\n",
    "    print(f\"Subdir: {s} Expecting: {res}\")\n",
    "    image_pname = f\"{s}/image.jpg\"\n",
    "    img_data = base64.b64encode(Path(image_pname).read_bytes()).decode()\n",
    "    res, msg = MODEL_INTERFACE.locate(img_data, o_desc[o], c_desc[c])\n",
    "    img = Image.open(image_pname)\n",
    "    w, h = img.size\n",
    "    display(img.resize((int(w / 4), int(h / 4))))\n",
    "    print(\"Inference result: \", \"yes\" if res else \"no\")\n",
    "    print(\"Location: \", msg if msg is not None else \"N/A\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "for c in chans:\n",
    "    for o in objs:\n",
    "        dir = f\"{dataset}/{c}/{o}\"\n",
    "        subdirs = [f.path for f in os.scandir(dir) if f.is_dir()]\n",
    "        true_pos = False\n",
    "        false_pos = False\n",
    "        for s in subdirs:\n",
    "            if true_pos and false_pos:\n",
    "                break\n",
    "            if os.path.exists(f\"{s}/skip\"):\n",
    "                continue\n",
    "            if not false_pos and os.path.exists(f\"{s}/no\"):\n",
    "                test_inf(s, c, o, \"no\")\n",
    "                false_pos = True\n",
    "            if not true_pos and not os.path.exists(f\"{s}/no\"):\n",
    "                test_inf(s, c, o, \"yes\")\n",
    "                true_pos = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do full walk through using the vLLM served model.\n",
    "# Show the location description for all positive cases, also show errors.\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import base64\n",
    "\n",
    "def check_inf(s, c, o, exp_res):\n",
    "    image_pname = f\"{s}/image.jpg\"\n",
    "    img_data = base64.b64encode(Path(image_pname).read_bytes()).decode()\n",
    "    res, msg = MODEL_INTERFACE.locate(img_data, o_desc[o], c_desc[c])\n",
    "    error = 0\n",
    "    if (res and exp_res != 'yes') or (not res and exp_res != 'no'):\n",
    "        error = 1\n",
    "    if res or error > 0:\n",
    "        msg = \"none\" if msg is None else msg.replace(\"\\n\", \"\")\n",
    "        print(f\"Subdir: {s} Expecting: {exp_res} {'(error)' if error > 0 else '(ok)'} Location: {msg}\")\n",
    "    return error\n",
    "\n",
    "total = 0\n",
    "total_16b_no = 0\n",
    "total_16b_yes = 0\n",
    "for c in chans:\n",
    "    for o in objs:\n",
    "        dir = f\"{dataset}/{c}/{o}\"\n",
    "        subdirs = [f.path for f in os.scandir(dir) if f.is_dir()]\n",
    "        for s in subdirs:\n",
    "            if os.path.exists(f\"{s}/skip\"):\n",
    "                continue\n",
    "            if os.path.exists(f\"{s}/no\"):\n",
    "                cur_err_no = check_inf(s, c, o, \"no\")\n",
    "                cur_err_yes = 0\n",
    "            else:\n",
    "                cur_err_yes = check_inf(s, c, o, \"yes\")\n",
    "                cur_err_no = 0\n",
    "            total_16b_no += cur_err_no\n",
    "            total_16b_yes += cur_err_yes\n",
    "            total += 1\n",
    "            _, dn = os.path.split(s)\n",
    "            #print(f\"\\rTotal:{total} err16b_no:{cur_err_no} err16b_yes:{cur_err_yes} Subdir:{c}/{o}/{dn}      \", end=\"\")\n",
    "\n",
    "print(f\"\\n----------------------------------------------------------\")\n",
    "print(f\"\\nSummary: out of {total} Error on No: {total_16b_no}, Error on Yes:{total_16b_yes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the accuracy has improved. It's unclear why no errors in the confusion matrix, but under vLLM there are a few errors.</br>\n",
    "Those error samples are false negatives (with the object being looked for reasonably difficult to recognize).</br>\n",
    "It could be some of the true positives that were eliminated when balancing the dataset.</br>\n",
    "Maybe it's related to the model conversion to f16 for vLLM or vLLM image processing pipeline?</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"production\" testing has been going well for 3 days.</br>\n",
    "On the 4th it generated a series of false positives (for a \"person in the driver's seat in the car\").</br>\n",
    "Then, continued without errors (for now for another 3 days).</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
