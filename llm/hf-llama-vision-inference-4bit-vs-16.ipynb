{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Local infernece with hf transformers LLAMA3.2</h1>\n",
    "This notebook uses the hf transformers LLAMA3.2 16-bit and 4-bit quantized models for inference.</br>\n",
    "This is done to confirm that when running inference locally we see the same errors as with OLLAMA,</br>\n",
    "and to compare the results of the 16-bit and 4-bit quantized models.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import MllamaForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "hf_model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for better performance\n",
    "    bnb_4bit_quant_type=\"nf4\"  # Specify quantization type (e.g., NF4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "model = None\n",
    "tokenizer = None\n",
    "processor = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4-bit quantized model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "processor = AutoProcessor.from_pretrained(hf_model_name)\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    hf_model_name,\n",
    "    device_map=\"auto\",  # Automatically map model layers to available GPUs\n",
    "    torch_dtype=torch.bfloat16,  # Use float16 precision for faster inference\n",
    "    quantization_config=quantization_config,  # Pass the quantization configuration\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model16 = MllamaForConditionalGeneration.from_pretrained(\n",
    "    hf_model_name,\n",
    "    device_map=\"auto\",  # Automatically map model layers to available GPUs\n",
    "    torch_dtype=torch.bfloat16,  # Use float16 precision for faster inference\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adding LoRA adapters from fine tuning session\n",
    "from peft import PeftModel\n",
    "lora_adapter_path = f\"{os.getcwd()}/trained/watchman_model_last_loras\"\n",
    "print(f\"Path to the adapters: {lora_adapter_path}\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_adapter_path,\n",
    "    is_trainable=False,  # Set to False for inference; True if you want to continue fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vars for finding train data\n",
    "dataset = \"../.data/dataset\" # dataset folder location\n",
    "chans = [\"porch\"] # list of channels to load the data for\n",
    "objs = [\"person\"] # list of objects to load the data for\n",
    "model_name = \"ollama-complex\" # model interface name to use for inference experimentation\n",
    "c_desc = {\n",
    "    \"porch\": \"Porch\",\n",
    "}\n",
    "o_desc = {\n",
    "    \"person\": \"a person\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model interfaces we can use for testing\n",
    "print(f\"Working dir: {os.getcwd()}\")\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../orchestrator\"))\n",
    "from shared_settings import *\n",
    "from model_interfaces import *\n",
    "print(MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the model_name interface (the connection refused error can be ignored, only need the prompt from the MODEL_ITERFACE)\n",
    "MODEL_INTERFACE = MODELS[model_name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example image as input in base64 format\n",
    "def load_image_from_base64(base64_string):\n",
    "    # Decode the base64 string\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "# Define an example image as input in base64 format\n",
    "def load_image_from_file(pname):\n",
    "    # Load the image\n",
    "    image = Image.open(pname).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def test_inf(model, s, c, o, res):\n",
    "    print(f\"Model: {model.device} Subdir: {s} Expecting: {res}\")\n",
    "    image_pname = f\"{s}/image.jpg\"\n",
    "    # Define the input prompt for completion\n",
    "    prompt = MODEL_INTERFACE.gen_detect_prompt(o_desc[o], c_desc[c])\n",
    "    # Combine the image w/ the text input (tokenizes text too)\n",
    "    image = load_image_from_file(image_pname)\n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # Generate the model's completion\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,  # Adjust max_length as needed\n",
    "        temperature=None,  # Adjust temperature for creativity\n",
    "        top_p=None,  # Use nucleus sampling for diversity\n",
    "        do_sample=False  # Enable sampling for non-deterministic output\n",
    "    )\n",
    "    # Decode output\n",
    "    rsp_prefix = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    img = Image.open(image_pname)\n",
    "    w, h = img.size\n",
    "    display(img.resize((int(w / 4), int(h / 4))))\n",
    "    print(\"Inference result: \", response[len(rsp_prefix):])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try inference for both 4 and 16 bit models on one true and one false positive sample from the datatset\n",
    "for c in chans:\n",
    "    for o in objs:\n",
    "        dir = f\"{dataset}/{c}/{o}\"\n",
    "        subdirs = [f.path for f in os.scandir(dir) if f.is_dir()]\n",
    "        true_pos = False\n",
    "        false_pos = False\n",
    "        for s in subdirs:\n",
    "            if true_pos and false_pos:\n",
    "                break\n",
    "            if os.path.exists(f\"{s}/skip\"):\n",
    "                continue\n",
    "            if not false_pos and os.path.exists(f\"{s}/no\"):\n",
    "                test_inf(model, s, c, o, \"no\")\n",
    "                test_inf(model16, s, c, o, \"no\")\n",
    "                false_pos = True\n",
    "            if not true_pos and not os.path.exists(f\"{s}/no\"):\n",
    "                test_inf(model, s, c, o, \"yes\")\n",
    "                test_inf(model16, s, c, o, \"yes\")\n",
    "                true_pos = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the 4-bit quantized model does not generate the false positives where the 16bit one does.\n",
    "Let's run a full sweep over the collected data to compare how one performs vs the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inf(model, s, c, o, res):\n",
    "    image_pname = f\"{s}/image.jpg\"\n",
    "    # Define the input prompt for completion\n",
    "    prompt = MODEL_INTERFACE.gen_detect_prompt(o_desc[o], c_desc[c])\n",
    "    # Combine the image w/ the text input (tokenizes text too)\n",
    "    image = load_image_from_file(image_pname)\n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # Generate the model's completion\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,  # Adjust max_length as needed\n",
    "        temperature=None,  # Adjust temperature for creativity\n",
    "        top_p=None,  # Use nucleus sampling for diversity\n",
    "        do_sample=False  # Enable sampling for non-deterministic output\n",
    "    )\n",
    "    # Decode output\n",
    "    rsp_prefix = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    res_idx = response[len(rsp_prefix):].lower().find(res)\n",
    "    return 1 if res_idx < 0 else 0\n",
    "\n",
    "total = 0\n",
    "total_4b_no = 0\n",
    "total_16b_no = 0\n",
    "total_4b_yes = 0\n",
    "total_16b_yes = 0\n",
    "for c in chans:\n",
    "    for o in objs:\n",
    "        dir = f\"{dataset}/{c}/{o}\"\n",
    "        subdirs = [f.path for f in os.scandir(dir) if f.is_dir()]\n",
    "        for s in subdirs:\n",
    "            if os.path.exists(f\"{s}/skip\"):\n",
    "                continue\n",
    "            if os.path.exists(f\"{s}/no\"):\n",
    "                cur_err_no = check_inf(model, s, c, o, \"no\")\n",
    "                cur_err16_no = check_inf(model16, s, c, o, \"no\")\n",
    "                cur_err_yes = 0\n",
    "                cur_err16_yes = 0\n",
    "            else:\n",
    "                cur_err_yes = check_inf(model, s, c, o, \"yes\")\n",
    "                cur_err16_yes = check_inf(model16, s, c, o, \"yes\")\n",
    "                cur_err_no = 0\n",
    "                cur_err16_no = 0\n",
    "            total_4b_no += cur_err_no\n",
    "            total_16b_no += cur_err16_no\n",
    "            total_4b_yes += cur_err_yes\n",
    "            total_16b_yes += cur_err16_yes\n",
    "            total += 1\n",
    "            _, dn = os.path.split(s)\n",
    "            print(f\"\\rTotal:{total} err16b_no:{cur_err16_no} err4b_no:{cur_err_no} err16b_yes:{cur_err16_yes} err4b_yes:{cur_err_yes} Subdir:{c}/{o}/{dn}      \", end=(\"\\n\" if not (cur_err16_no == cur_err_no and cur_err16_yes == cur_err_yes)  else \"\"))\n",
    "\n",
    "print(f\"\\n----------------------------------------------------------\")\n",
    "print(f\"\\nSummary: out of {total} false No 4bit:{total_4b_no} 16bit:{total_16b_no}, false Yes 4bit:{total_4b_yes} 16bit:{total_16b_yes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 4bit quantized model was a bit better at negative detection. It missed some true positive cases though.</br>\n",
    "It's quite possible that it's just the side effect of the dataset only containing all the \"positives\" the 16 bit model detected (including errors).</br>\n",
    "Perhaps the 4bit could have made more false positive mistakes, just on different images (not represented here since 16bit rejected them correctly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on the GPU parallelizm, the MllamaForConditionalGeneration.from_pretrained(..., device_map=\"auto\", ...) should not be used with .to(device) as it's managing GPUs internally. \"The model weights are not tied...\" message can be ignored (it's incorrect)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
